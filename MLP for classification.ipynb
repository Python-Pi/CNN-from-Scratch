{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification on fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing function\n",
    "- Loads the datatset from CSV and segregates it into training and testing data\n",
    "- Also, normalizes the data for pixel values to be in the range of 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from CSV\n",
    "def load_fashion_mnist(csv_path):\n",
    "    data = pd.read_csv(csv_path).values  \n",
    "    labels = data[:, 0]  \n",
    "    images = data[:, 1:] \n",
    "\n",
    "    # Normalize pixel values to [0,1]\n",
    "    images = images.astype(np.float32) / 255.0\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information on fashin mnist dataset\n",
    "- The training dataset contains 60,000 images and the test dataset contains 10,000 images.\n",
    "- Each image is 28x28 pixels which are converted into an array of 784 elements.\n",
    "- Each image is labeled with a number from 0 to 9 which represents the class of the image.\n",
    "- The validation dataset is obtained as 20% of the training dataset.\n",
    "\n",
    "### Data size\n",
    "* Training dataset - 48,000 images\n",
    "* Validation dataset - 12,000 images\n",
    "* Test dataset - 10,000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (48000, 784), Validation: (12000, 784), Test: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Load train and test datasets\n",
    "train_images, train_labels = load_fashion_mnist(\"datasets/fashion-mnist_train.csv\")\n",
    "test_images, test_labels = load_fashion_mnist(\"datasets/fashion-mnist_test.csv\")\n",
    "\n",
    "# Split train into (train + validation)\n",
    "num_train = int(0.8 * train_images.shape[0])\n",
    "val_images, val_labels = train_images[num_train:], train_labels[num_train:]\n",
    "train_images, train_labels = train_images[:num_train], train_labels[:num_train]\n",
    "\n",
    "print(f\"Train: {train_images.shape}, Validation: {val_images.shape}, Test: {test_images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting labels into one-hot encoding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes=10):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_one_hot = one_hot_encode(train_labels)\n",
    "y_val_one_hot = one_hot_encode(val_labels)\n",
    "y_test_one_hot = one_hot_encode(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP for Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layer_sizes, activation='relu', dropout_rate=0.0, optimizer='sgd', learning_rate=0.01, momentum=0.9):\n",
    "        \"\"\"\n",
    "        Initializes an MLP with a variable number of layers.\n",
    "\n",
    "        :param layer_sizes: List of sizes of each layer (including input & output).\n",
    "                            Example: [784, 128, 64, 10] -> 2 hidden layers.\n",
    "        :param activation: Activation function ('relu', 'leaky_relu', 'tanh', 'sigmoid').\n",
    "        :param dropout_rate: Dropout rate (0.0 means no dropout, 0.5 means 50% dropout).\n",
    "        :param optimizer: Optimization method ('sgd' or 'momentum').\n",
    "        :param learning_rate: Learning rate for weight updates.\n",
    "        :param momentum: Momentum term for gradient updates (only for momentum optimizer).\n",
    "        \"\"\"\n",
    "        self.num_layers = len(layer_sizes) - 1\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activation_func = self.get_activation_function(activation)\n",
    "        self.activation_derivative = self.get_activation_derivative(activation)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # Initialize weight velocities for momentum-based optimization\n",
    "        self.velocity_W = [np.zeros((layer_sizes[i], layer_sizes[i + 1])) for i in range(self.num_layers)]\n",
    "        self.velocity_b = [np.zeros((1, layer_sizes[i + 1])) for i in range(self.num_layers)]\n",
    "\n",
    "        # Xavier/He Initialization for weights\n",
    "        for i in range(self.num_layers):\n",
    "            limit = np.sqrt(2 / layer_sizes[i])  # He Initialization\n",
    "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * limit)\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i + 1])))\n",
    "\n",
    "    def get_activation_function(self, activation):\n",
    "        \"\"\" Returns the activation function. \"\"\"\n",
    "        if activation == 'relu':\n",
    "            return lambda x: np.maximum(0, x)\n",
    "        elif activation == 'leaky_relu':\n",
    "            return lambda x: np.where(x > 0, x, 0.01 * x)\n",
    "        elif activation == 'tanh':\n",
    "            return lambda x: np.tanh(x)\n",
    "        elif activation == 'sigmoid':\n",
    "            return lambda x: 1 / (1 + np.exp(-x))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def get_activation_derivative(self, activation):\n",
    "        \"\"\" Returns the derivative of the activation function. \"\"\"\n",
    "        if activation == 'relu':\n",
    "            return lambda x: (x > 0).astype(float)\n",
    "        elif activation == 'leaky_relu':\n",
    "            return lambda x: np.where(x > 0, 1, 0.01)\n",
    "        elif activation == 'tanh':\n",
    "            return lambda x: 1 - np.tanh(x) ** 2\n",
    "        elif activation == 'sigmoid':\n",
    "            sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "            return lambda x: sigmoid(x) * (1 - sigmoid(x))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\" Computes softmax activation for the output layer. \"\"\"\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy_loss(self, y_true, y_pred):\n",
    "        \"\"\" Computes cross-entropy loss. \"\"\"\n",
    "        num_samples = y_true.shape[0]\n",
    "        return -np.sum(y_true * np.log(y_pred + 1e-9)) / num_samples\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"\n",
    "        Forward propagation through multiple layers with optional dropout.\n",
    "        \"\"\"\n",
    "        self.activations = [X]\n",
    "        self.z_values = []\n",
    "        self.drop_masks = []\n",
    "\n",
    "        for i in range(self.num_layers - 1):  # Hidden layers\n",
    "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
    "            self.z_values.append(z)\n",
    "\n",
    "            a = self.activation_func(z)\n",
    "\n",
    "            # Apply dropout during training\n",
    "            if training and self.dropout_rate > 0:\n",
    "                mask = (np.random.rand(*a.shape) > self.dropout_rate) / (1.0 - self.dropout_rate)\n",
    "                self.drop_masks.append(mask)\n",
    "                a *= mask\n",
    "            else:\n",
    "                self.drop_masks.append(None)\n",
    "\n",
    "            self.activations.append(a)\n",
    "\n",
    "        # Output layer with softmax (no dropout)\n",
    "        z_out = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        self.z_values.append(z_out)\n",
    "        self.activations.append(self.softmax(z_out))\n",
    "\n",
    "        return self.activations[-1]\n",
    "\n",
    "    def backward(self, y_true):\n",
    "        \"\"\"\n",
    "        Backpropagation through multiple layers.\n",
    "        \"\"\"\n",
    "        num_samples = y_true.shape[0]\n",
    "        grads_W = [None] * self.num_layers\n",
    "        grads_b = [None] * self.num_layers\n",
    "\n",
    "        # Compute gradients for output layer\n",
    "        dZ = self.activations[-1] - y_true\n",
    "        grads_W[-1] = np.dot(self.activations[-2].T, dZ) / num_samples\n",
    "        grads_b[-1] = np.sum(dZ, axis=0, keepdims=True) / num_samples\n",
    "\n",
    "        # Backpropagate through hidden layers\n",
    "        for i in range(self.num_layers - 2, -1, -1):\n",
    "            dA = np.dot(dZ, self.weights[i + 1].T)\n",
    "\n",
    "            if self.drop_masks[i] is not None:  # Apply dropout mask during backprop\n",
    "                dA *= self.drop_masks[i]\n",
    "\n",
    "            dZ = dA * self.activation_derivative(self.z_values[i])\n",
    "            grads_W[i] = np.dot(self.activations[i].T, dZ) / num_samples\n",
    "            grads_b[i] = np.sum(dZ, axis=0, keepdims=True) / num_samples\n",
    "\n",
    "        # Update weights and biases with optimizer\n",
    "        for i in range(self.num_layers):\n",
    "            if self.optimizer == 'momentum':\n",
    "                self.velocity_W[i] = self.momentum * self.velocity_W[i] - self.learning_rate * grads_W[i]\n",
    "                self.velocity_b[i] = self.momentum * self.velocity_b[i] - self.learning_rate * grads_b[i]\n",
    "                self.weights[i] += self.velocity_W[i]\n",
    "                self.biases[i] += self.velocity_b[i]\n",
    "            else:\n",
    "                self.weights[i] -= self.learning_rate * grads_W[i]\n",
    "                self.biases[i] -= self.learning_rate * grads_b[i]\n",
    "\n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, epochs=100, batch_size=64):\n",
    "        \"\"\"\n",
    "        Train the model using mini-batch stochastic gradient descent (SGD).\n",
    "        \"\"\"\n",
    "        num_samples = X_train.shape[0]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_train, y_train = X_train[indices], y_train[indices]\n",
    "\n",
    "            for i in range(0, num_samples, batch_size):\n",
    "                X_batch = X_train[i:i + batch_size]\n",
    "                y_batch = y_train[i:i + batch_size]\n",
    "\n",
    "                y_pred = self.forward(X_batch, training=True)\n",
    "                self.backward(y_batch)\n",
    "\n",
    "            # Evaluate on validation data\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_pred = self.forward(X_val, training=False)\n",
    "                val_loss = self.cross_entropy_loss(y_val, y_val_pred)\n",
    "                val_accuracy = np.mean(np.argmax(y_val_pred, axis=1) == np.argmax(y_val, axis=1)) * 100\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Val Loss: {val_loss:.4f} - Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predicts labels for input data. \"\"\"\n",
    "        return np.argmax(self.forward(X, training=False), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting under different conditions\n",
    "- Varying the number of hidden layers\n",
    "- Varying the dropout rate\n",
    "- Varying the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp1 = MLP(layer_sizes=[784, 20, 10], activation='relu', dropout_rate=0.2)\n",
    "mlp2 = MLP(layer_sizes=[784, 128, 10], activation='relu', dropout_rate=0.2)\n",
    "mlp3 = MLP(layer_sizes=[784, 128, 64, 10], activation='tanh', dropout_rate=0.3)\n",
    "mlp4 = MLP(layer_sizes=[784, 256, 128, 64, 10], activation='relu', dropout_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Val Loss: 0.8099 - Val Acc: 73.78%\n",
      "Epoch 2/20 - Val Loss: 0.6698 - Val Acc: 78.62%\n",
      "Epoch 3/20 - Val Loss: 0.6118 - Val Acc: 79.97%\n",
      "Epoch 4/20 - Val Loss: 0.5764 - Val Acc: 81.16%\n",
      "Epoch 5/20 - Val Loss: 0.5528 - Val Acc: 81.71%\n",
      "Epoch 6/20 - Val Loss: 0.5335 - Val Acc: 82.50%\n",
      "Epoch 7/20 - Val Loss: 0.5184 - Val Acc: 82.67%\n",
      "Epoch 8/20 - Val Loss: 0.5098 - Val Acc: 82.92%\n",
      "Epoch 9/20 - Val Loss: 0.5034 - Val Acc: 82.87%\n",
      "Epoch 10/20 - Val Loss: 0.4942 - Val Acc: 83.03%\n",
      "Epoch 11/20 - Val Loss: 0.4849 - Val Acc: 83.79%\n",
      "Epoch 12/20 - Val Loss: 0.4817 - Val Acc: 83.81%\n",
      "Epoch 13/20 - Val Loss: 0.4725 - Val Acc: 84.12%\n",
      "Epoch 14/20 - Val Loss: 0.4725 - Val Acc: 83.75%\n",
      "Epoch 15/20 - Val Loss: 0.4648 - Val Acc: 83.90%\n",
      "Epoch 16/20 - Val Loss: 0.4611 - Val Acc: 84.12%\n",
      "Epoch 17/20 - Val Loss: 0.4589 - Val Acc: 84.23%\n",
      "Epoch 18/20 - Val Loss: 0.4577 - Val Acc: 84.25%\n",
      "Epoch 19/20 - Val Loss: 0.4538 - Val Acc: 84.53%\n",
      "Epoch 20/20 - Val Loss: 0.4475 - Val Acc: 84.67%\n"
     ]
    }
   ],
   "source": [
    "# Train model: train(self, X_train, y_train, X_val=None, y_val=None, epochs=100, batch_size=64)\n",
    "mlp1.train(train_images, y_train_one_hot, val_images, y_val_one_hot, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 84.67%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "predictions = mlp1.predict(val_images)\n",
    "accuracy = np.mean(predictions == val_labels) * 100\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using CNN from pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, pooling_type='max'):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.pooling_type = pooling_type\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=2, padding=1)\n",
    "        # self.conv3 = nn.Conv2d(64, 128, kernel_size=2, padding=1)\n",
    "        # self.conv4 = nn.Conv2d(128, 128, kernel_size=2, padding=1)\n",
    "        # self.conv5 = nn.Conv2d(128, 256, kernel_size=2, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self._pooling_layer(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self._pooling_layer(x)\n",
    "        \n",
    "        # x = F.relu(self.conv3(x))\n",
    "        # x = self._pooling_layer(x)\n",
    "        \n",
    "        # x = F.relu(self.conv4(x))\n",
    "        # x = self._pooling_layer(x)\n",
    "        \n",
    "        # x = F.relu(self.conv5(x))\n",
    "        # x = self._pooling_layer(x)\n",
    "        \n",
    "        if self.pooling_type == 'global_avg':\n",
    "            x = F.adaptive_avg_pool2d(x, (1, 1))  \n",
    "            x = x.view(x.size(0), -1)  \n",
    "        else:\n",
    "            x = x.view(x.size(0), -1)  \n",
    "        \n",
    "        return x \n",
    "\n",
    "    def _pooling_layer(self, x):\n",
    "        if self.pooling_type == 'max':\n",
    "            return F.max_pool2d(x, 2)\n",
    "        elif self.pooling_type == 'avg':\n",
    "            return F.avg_pool2d(x, 2)\n",
    "        else:\n",
    "            return x  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: torch.Size([48000, 3136])\n"
     ]
    }
   ],
   "source": [
    "cnn_model = CNNModel(pooling_type='max')\n",
    "X_train = train_images.reshape(-1, 1, 28, 28)  \n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_val = val_images.reshape(-1, 1, 28, 28)\n",
    "X_val = torch.from_numpy(X_val).float()\n",
    "features_train = cnn_model(X_train)\n",
    "features_val = cnn_model(X_val)\n",
    "\n",
    "print(\"Extracted features shape:\", features_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training MLP on extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Val Loss: 1.1674 - Val Acc: 68.93%\n",
      "Epoch 2/20 - Val Loss: 0.9282 - Val Acc: 70.50%\n",
      "Epoch 3/20 - Val Loss: 0.8281 - Val Acc: 73.32%\n",
      "Epoch 4/20 - Val Loss: 0.7697 - Val Acc: 74.33%\n",
      "Epoch 5/20 - Val Loss: 0.7311 - Val Acc: 75.36%\n",
      "Epoch 6/20 - Val Loss: 0.7030 - Val Acc: 75.82%\n",
      "Epoch 7/20 - Val Loss: 0.6711 - Val Acc: 76.68%\n",
      "Epoch 8/20 - Val Loss: 0.6547 - Val Acc: 76.88%\n",
      "Epoch 9/20 - Val Loss: 0.6371 - Val Acc: 77.67%\n",
      "Epoch 10/20 - Val Loss: 0.6198 - Val Acc: 78.08%\n",
      "Epoch 11/20 - Val Loss: 0.6089 - Val Acc: 78.40%\n",
      "Epoch 12/20 - Val Loss: 0.5956 - Val Acc: 78.86%\n",
      "Epoch 13/20 - Val Loss: 0.5817 - Val Acc: 78.95%\n",
      "Epoch 14/20 - Val Loss: 0.5747 - Val Acc: 79.37%\n",
      "Epoch 15/20 - Val Loss: 0.5725 - Val Acc: 79.53%\n",
      "Epoch 16/20 - Val Loss: 0.5576 - Val Acc: 79.88%\n",
      "Epoch 17/20 - Val Loss: 0.5517 - Val Acc: 80.04%\n",
      "Epoch 18/20 - Val Loss: 0.5493 - Val Acc: 79.80%\n",
      "Epoch 19/20 - Val Loss: 0.5343 - Val Acc: 80.64%\n",
      "Epoch 20/20 - Val Loss: 0.5371 - Val Acc: 80.58%\n"
     ]
    }
   ],
   "source": [
    "features_flat_train = features_train.view(features_train.size(0), -1)\n",
    "features_flat_val = features_val.view(features_val.size(0), -1)\n",
    "\n",
    "mlp_model = MLP(layer_sizes=[features_flat_train.shape[1], 20, 10], activation='relu', dropout_rate=0.2)\n",
    "mlp_model.train(features_flat_train.detach().numpy(), y_train_one_hot, features_flat_val.detach().numpy(), y_val_one_hot,  epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 81.36%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "X_test = test_images.reshape(-1, 1, 28, 28)\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "features_test = cnn_model(X_test)\n",
    "\n",
    "features_flat_test = features_test.view(features_test.size(0), -1)\n",
    "predictions = mlp_model.predict(features_flat_test.detach().numpy())\n",
    "accuracy = np.mean(predictions == test_labels) * 100\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
