{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification on fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing function\n",
    "- Loads the datatset from CSV and segregates it into training and testing data\n",
    "- Also, normalizes the data for pixel values to be in the range of 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from CSV\n",
    "def load_fashion_mnist(csv_path):\n",
    "    data = pd.read_csv(csv_path).values  \n",
    "    labels = data[:, 0]  \n",
    "    images = data[:, 1:] \n",
    "\n",
    "    # Normalize pixel values to [0,1]\n",
    "    images = images.astype(np.float32) / 255.0\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information on fashin mnist dataset\n",
    "- The training dataset contains 60,000 images and the test dataset contains 10,000 images.\n",
    "- Each image is 28x28 pixels which are converted into an array of 784 elements.\n",
    "- Each image is labeled with a number from 0 to 9 which represents the class of the image.\n",
    "- The validation dataset is obtained as 20% of the training dataset.\n",
    "\n",
    "### Data size\n",
    "* Training dataset - 48,000 images\n",
    "* Validation dataset - 12,000 images\n",
    "* Test dataset - 10,000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (48000, 784), Validation: (12000, 784), Test: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Load train and test datasets\n",
    "train_images, train_labels = load_fashion_mnist(\"datasets/fashion-mnist_train.csv\")\n",
    "test_images, test_labels = load_fashion_mnist(\"datasets/fashion-mnist_test.csv\")\n",
    "\n",
    "# Split train into (train + validation)\n",
    "num_train = int(0.8 * train_images.shape[0])\n",
    "val_images, val_labels = train_images[num_train:], train_labels[num_train:]\n",
    "train_images, train_labels = train_images[:num_train], train_labels[:num_train]\n",
    "\n",
    "print(f\"Train: {train_images.shape}, Validation: {val_images.shape}, Test: {test_images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting labels into one-hot encoding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes=10):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_one_hot = one_hot_encode(train_labels)\n",
    "y_val_one_hot = one_hot_encode(val_labels)\n",
    "y_test_one_hot = one_hot_encode(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP for Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# class MLP:\n",
    "#     def __init__(self, layer_sizes, activation='relu', dropout_rate=0.0):\n",
    "#         \"\"\"\n",
    "#         Initializes an MLP with a variable number of layers.\n",
    "        \n",
    "#         :param layer_sizes: List containing sizes of each layer (including input & output).\n",
    "#                             Example: [784, 128, 64, 10] -> 2 hidden layers (128 and 64 neurons).\n",
    "#         :param activation: Activation function ('relu', 'leaky_relu', 'tanh', 'gelu').\n",
    "#         :param dropout_rate: Dropout rate (0.0 means no dropout, 0.5 means 50% dropout).\n",
    "#         \"\"\"\n",
    "#         self.num_layers = len(layer_sizes) - 1\n",
    "#         self.weights = []\n",
    "#         self.biases = []\n",
    "#         self.dropout_rate = dropout_rate\n",
    "#         self.activation_func = self.get_activation_function(activation)\n",
    "#         self.activation_derivative = self.get_activation_derivative(activation)\n",
    "\n",
    "#         # Xavier Initialization\n",
    "#         for i in range(self.num_layers):\n",
    "#             self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]) / np.sqrt(layer_sizes[i]))\n",
    "#             self.biases.append(np.zeros((1, layer_sizes[i + 1])))\n",
    "\n",
    "#     def get_activation_function(self, activation):\n",
    "#         \"\"\"\n",
    "#         Returns the appropriate activation function.\n",
    "#         \"\"\"\n",
    "#         if activation == 'relu':\n",
    "#             return lambda x: np.maximum(0, x)\n",
    "#         elif activation == 'leaky_relu':\n",
    "#             return lambda x: np.where(x > 0, x, 0.01 * x)\n",
    "#         elif activation == 'tanh':\n",
    "#             return lambda x: np.tanh(x)\n",
    "#         elif activation == 'gelu':\n",
    "#             return lambda x: 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "#         else:\n",
    "#             raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "#     def get_activation_derivative(self, activation):\n",
    "#         \"\"\"\n",
    "#         Returns the derivative of the activation function.\n",
    "#         \"\"\"\n",
    "#         if activation == 'relu':\n",
    "#             return lambda x: (x > 0).astype(float)\n",
    "#         elif activation == 'leaky_relu':\n",
    "#             return lambda x: np.where(x > 0, 1, 0.01)\n",
    "#         elif activation == 'tanh':\n",
    "#             return lambda x: 1 - np.tanh(x)**2\n",
    "#         elif activation == 'gelu':\n",
    "#             return lambda x: 0.5 * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3))) + \\\n",
    "#                             0.5 * x * (1 - np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3))**2) * \\\n",
    "#                             (np.sqrt(2 / np.pi) * (1 + 3 * 0.044715 * x**2))\n",
    "#         else:\n",
    "#             raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "#     def softmax(self, x):\n",
    "#         exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "#         return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "#     def cross_entropy_loss(self, y_true, y_pred):\n",
    "#         num_samples = y_true.shape[0]\n",
    "#         return -np.sum(y_true * np.log(y_pred + 1e-9)) / num_samples\n",
    "\n",
    "#     def cross_entropy_derivative(self, y_true, y_pred):\n",
    "#         return y_pred - y_true\n",
    "\n",
    "#     def forward(self, X, training=True):\n",
    "#         \"\"\"\n",
    "#         Forward propagation through multiple layers with dropout.\n",
    "#         \"\"\"\n",
    "#         self.activations = [X]\n",
    "#         self.z_values = []\n",
    "#         self.drop_masks = []\n",
    "\n",
    "#         for i in range(self.num_layers - 1):  # Hidden layers\n",
    "#             z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
    "#             self.z_values.append(z)\n",
    "\n",
    "#             a = self.activation_func(z)\n",
    "\n",
    "#             # Apply dropout during training\n",
    "#             if training and self.dropout_rate > 0:\n",
    "#                 mask = (np.random.rand(*a.shape) > self.dropout_rate) / (1.0 - self.dropout_rate)\n",
    "#                 self.drop_masks.append(mask)\n",
    "#                 a *= mask\n",
    "#             else:\n",
    "#                 self.drop_masks.append(None)\n",
    "\n",
    "#             self.activations.append(a)\n",
    "\n",
    "#         # Output layer with softmax (no dropout)\n",
    "#         z_out = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "#         self.z_values.append(z_out)\n",
    "#         self.activations.append(self.softmax(z_out))\n",
    "\n",
    "#         return self.activations[-1]\n",
    "\n",
    "#     def backward(self, y_true, learning_rate=0.01):\n",
    "#         \"\"\"\n",
    "#         Backpropagation through multiple layers.\n",
    "#         \"\"\"\n",
    "#         num_samples = y_true.shape[0]\n",
    "#         grads_W = [None] * self.num_layers\n",
    "#         grads_b = [None] * self.num_layers\n",
    "\n",
    "#         # Compute gradients for output layer\n",
    "#         dZ = self.cross_entropy_derivative(y_true, self.activations[-1])\n",
    "#         grads_W[-1] = np.dot(self.activations[-2].T, dZ) / num_samples\n",
    "#         grads_b[-1] = np.sum(dZ, axis=0, keepdims=True) / num_samples\n",
    "\n",
    "#         # Backpropagate through hidden layers\n",
    "#         for i in range(self.num_layers - 2, -1, -1):\n",
    "#             dA = np.dot(dZ, self.weights[i + 1].T)\n",
    "\n",
    "#             if self.drop_masks[i] is not None:  # Apply dropout mask during backprop\n",
    "#                 dA *= self.drop_masks[i]\n",
    "\n",
    "#             dZ = dA * self.activation_derivative(self.z_values[i])\n",
    "#             grads_W[i] = np.dot(self.activations[i].T, dZ) / num_samples\n",
    "#             grads_b[i] = np.sum(dZ, axis=0, keepdims=True) / num_samples\n",
    "\n",
    "#         # Update weights and biases\n",
    "#         for i in range(self.num_layers):\n",
    "#             self.weights[i] -= learning_rate * grads_W[i]\n",
    "#             self.biases[i] -= learning_rate * grads_b[i]\n",
    "\n",
    "#     def train(self, X_train, y_train, X_val=None, y_val=None, epochs=100, learning_rate=0.01, patience=10):\n",
    "#         best_val_loss = float('inf')\n",
    "#         patience_counter = 0\n",
    "\n",
    "#         for epoch in range(epochs):\n",
    "#             y_pred = self.forward(X_train, training=True)\n",
    "#             loss = self.cross_entropy_loss(y_train, y_pred)\n",
    "\n",
    "#             # Backpropagation step\n",
    "#             self.backward(y_train, learning_rate)\n",
    "\n",
    "#             if (epoch+1) % 10 == 0:\n",
    "#                 # Compute accuracy\n",
    "#                 y_true_labels = np.argmax(y_train, axis=1) \n",
    "#                 y_pred_labels = np.argmax(y_pred, axis=1)   \n",
    "#                 accuracy = np.mean(y_pred_labels == y_true_labels) * 100\n",
    "\n",
    "#                 print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "#             # Validation step\n",
    "#             if X_val is not None and y_val is not None:\n",
    "#                 y_val_pred = self.forward(X_val, training=False)\n",
    "#                 val_loss = self.cross_entropy_loss(y_val, y_val_pred)\n",
    "\n",
    "#                 if val_loss < best_val_loss:\n",
    "#                     best_val_loss = val_loss\n",
    "#                     patience_counter = 0\n",
    "#                 else:\n",
    "#                     patience_counter += 1\n",
    "\n",
    "#                 if patience_counter >= patience:\n",
    "#                     print(f\"Early stopping at epoch {epoch+1}\")\n",
    "#                     break\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         \"\"\"\n",
    "#         Predict labels for given input.\n",
    "#         \"\"\"\n",
    "#         return np.argmax(self.forward(X, training=False), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layer_sizes, activation='relu', dropout_rate=0.0, optimizer='sgd', learning_rate=0.01, momentum=0.9):\n",
    "        \"\"\"\n",
    "        Initializes an MLP with a variable number of layers.\n",
    "\n",
    "        :param layer_sizes: List of sizes of each layer (including input & output).\n",
    "                            Example: [784, 128, 64, 10] -> 2 hidden layers.\n",
    "        :param activation: Activation function ('relu', 'leaky_relu', 'tanh', 'sigmoid').\n",
    "        :param dropout_rate: Dropout rate (0.0 means no dropout, 0.5 means 50% dropout).\n",
    "        :param optimizer: Optimization method ('sgd' or 'momentum').\n",
    "        :param learning_rate: Learning rate for weight updates.\n",
    "        :param momentum: Momentum term for gradient updates (only for momentum optimizer).\n",
    "        \"\"\"\n",
    "        self.num_layers = len(layer_sizes) - 1\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activation_func = self.get_activation_function(activation)\n",
    "        self.activation_derivative = self.get_activation_derivative(activation)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # Initialize weight velocities for momentum-based optimization\n",
    "        self.velocity_W = [np.zeros((layer_sizes[i], layer_sizes[i + 1])) for i in range(self.num_layers)]\n",
    "        self.velocity_b = [np.zeros((1, layer_sizes[i + 1])) for i in range(self.num_layers)]\n",
    "\n",
    "        # Xavier/He Initialization for weights\n",
    "        for i in range(self.num_layers):\n",
    "            limit = np.sqrt(2 / layer_sizes[i])  # He Initialization\n",
    "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * limit)\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i + 1])))\n",
    "\n",
    "    def get_activation_function(self, activation):\n",
    "        \"\"\" Returns the activation function. \"\"\"\n",
    "        if activation == 'relu':\n",
    "            return lambda x: np.maximum(0, x)\n",
    "        elif activation == 'leaky_relu':\n",
    "            return lambda x: np.where(x > 0, x, 0.01 * x)\n",
    "        elif activation == 'tanh':\n",
    "            return lambda x: np.tanh(x)\n",
    "        elif activation == 'sigmoid':\n",
    "            return lambda x: 1 / (1 + np.exp(-x))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def get_activation_derivative(self, activation):\n",
    "        \"\"\" Returns the derivative of the activation function. \"\"\"\n",
    "        if activation == 'relu':\n",
    "            return lambda x: (x > 0).astype(float)\n",
    "        elif activation == 'leaky_relu':\n",
    "            return lambda x: np.where(x > 0, 1, 0.01)\n",
    "        elif activation == 'tanh':\n",
    "            return lambda x: 1 - np.tanh(x) ** 2\n",
    "        elif activation == 'sigmoid':\n",
    "            sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "            return lambda x: sigmoid(x) * (1 - sigmoid(x))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\" Computes softmax activation for the output layer. \"\"\"\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy_loss(self, y_true, y_pred):\n",
    "        \"\"\" Computes cross-entropy loss. \"\"\"\n",
    "        num_samples = y_true.shape[0]\n",
    "        return -np.sum(y_true * np.log(y_pred + 1e-9)) / num_samples\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"\n",
    "        Forward propagation through multiple layers with optional dropout.\n",
    "        \"\"\"\n",
    "        self.activations = [X]\n",
    "        self.z_values = []\n",
    "        self.drop_masks = []\n",
    "\n",
    "        for i in range(self.num_layers - 1):  # Hidden layers\n",
    "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
    "            self.z_values.append(z)\n",
    "\n",
    "            a = self.activation_func(z)\n",
    "\n",
    "            # Apply dropout during training\n",
    "            if training and self.dropout_rate > 0:\n",
    "                mask = (np.random.rand(*a.shape) > self.dropout_rate) / (1.0 - self.dropout_rate)\n",
    "                self.drop_masks.append(mask)\n",
    "                a *= mask\n",
    "            else:\n",
    "                self.drop_masks.append(None)\n",
    "\n",
    "            self.activations.append(a)\n",
    "\n",
    "        # Output layer with softmax (no dropout)\n",
    "        z_out = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        self.z_values.append(z_out)\n",
    "        self.activations.append(self.softmax(z_out))\n",
    "\n",
    "        return self.activations[-1]\n",
    "\n",
    "    def backward(self, y_true):\n",
    "        \"\"\"\n",
    "        Backpropagation through multiple layers.\n",
    "        \"\"\"\n",
    "        num_samples = y_true.shape[0]\n",
    "        grads_W = [None] * self.num_layers\n",
    "        grads_b = [None] * self.num_layers\n",
    "\n",
    "        # Compute gradients for output layer\n",
    "        dZ = self.activations[-1] - y_true\n",
    "        grads_W[-1] = np.dot(self.activations[-2].T, dZ) / num_samples\n",
    "        grads_b[-1] = np.sum(dZ, axis=0, keepdims=True) / num_samples\n",
    "\n",
    "        # Backpropagate through hidden layers\n",
    "        for i in range(self.num_layers - 2, -1, -1):\n",
    "            dA = np.dot(dZ, self.weights[i + 1].T)\n",
    "\n",
    "            if self.drop_masks[i] is not None:  # Apply dropout mask during backprop\n",
    "                dA *= self.drop_masks[i]\n",
    "\n",
    "            dZ = dA * self.activation_derivative(self.z_values[i])\n",
    "            grads_W[i] = np.dot(self.activations[i].T, dZ) / num_samples\n",
    "            grads_b[i] = np.sum(dZ, axis=0, keepdims=True) / num_samples\n",
    "\n",
    "        # Update weights and biases with optimizer\n",
    "        for i in range(self.num_layers):\n",
    "            if self.optimizer == 'momentum':\n",
    "                self.velocity_W[i] = self.momentum * self.velocity_W[i] - self.learning_rate * grads_W[i]\n",
    "                self.velocity_b[i] = self.momentum * self.velocity_b[i] - self.learning_rate * grads_b[i]\n",
    "                self.weights[i] += self.velocity_W[i]\n",
    "                self.biases[i] += self.velocity_b[i]\n",
    "            else:\n",
    "                self.weights[i] -= self.learning_rate * grads_W[i]\n",
    "                self.biases[i] -= self.learning_rate * grads_b[i]\n",
    "\n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, epochs=100, batch_size=64):\n",
    "        \"\"\"\n",
    "        Train the model using mini-batch stochastic gradient descent (SGD).\n",
    "        \"\"\"\n",
    "        num_samples = X_train.shape[0]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_train, y_train = X_train[indices], y_train[indices]\n",
    "\n",
    "            for i in range(0, num_samples, batch_size):\n",
    "                X_batch = X_train[i:i + batch_size]\n",
    "                y_batch = y_train[i:i + batch_size]\n",
    "\n",
    "                y_pred = self.forward(X_batch, training=True)\n",
    "                self.backward(y_batch)\n",
    "\n",
    "            # Evaluate on validation data\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_pred = self.forward(X_val, training=False)\n",
    "                val_loss = self.cross_entropy_loss(y_val, y_val_pred)\n",
    "                val_accuracy = np.mean(np.argmax(y_val_pred, axis=1) == np.argmax(y_val, axis=1)) * 100\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Val Loss: {val_loss:.4f} - Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predicts labels for input data. \"\"\"\n",
    "        return np.argmax(self.forward(X, training=False), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting under different conditions\n",
    "- Varying the number of hidden layers\n",
    "- Varying the dropout rate\n",
    "- Varying the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp1 = MLP(layer_sizes=[784, 20, 10], activation='relu', dropout_rate=0.2)\n",
    "mlp2 = MLP(layer_sizes=[784, 128, 64, 10], activation='tanh', dropout_rate=0.3)\n",
    "mlp3 = MLP(layer_sizes=[784, 256, 128, 64, 10], activation='relu', dropout_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Val Loss: 0.8185 - Val Acc: 73.47%\n",
      "Epoch 2/20 - Val Loss: 0.6702 - Val Acc: 78.29%\n",
      "Epoch 3/20 - Val Loss: 0.6088 - Val Acc: 80.28%\n",
      "Epoch 4/20 - Val Loss: 0.5723 - Val Acc: 81.00%\n",
      "Epoch 5/20 - Val Loss: 0.5520 - Val Acc: 81.67%\n",
      "Epoch 6/20 - Val Loss: 0.5339 - Val Acc: 82.37%\n",
      "Epoch 7/20 - Val Loss: 0.5169 - Val Acc: 82.56%\n",
      "Epoch 8/20 - Val Loss: 0.5077 - Val Acc: 82.79%\n",
      "Epoch 9/20 - Val Loss: 0.4998 - Val Acc: 82.95%\n",
      "Epoch 10/20 - Val Loss: 0.4948 - Val Acc: 83.20%\n",
      "Epoch 11/20 - Val Loss: 0.4864 - Val Acc: 83.29%\n",
      "Epoch 12/20 - Val Loss: 0.4828 - Val Acc: 83.38%\n",
      "Epoch 13/20 - Val Loss: 0.4754 - Val Acc: 83.43%\n",
      "Epoch 14/20 - Val Loss: 0.4725 - Val Acc: 83.67%\n",
      "Epoch 15/20 - Val Loss: 0.4681 - Val Acc: 83.79%\n",
      "Epoch 16/20 - Val Loss: 0.4641 - Val Acc: 83.93%\n",
      "Epoch 17/20 - Val Loss: 0.4622 - Val Acc: 84.17%\n",
      "Epoch 18/20 - Val Loss: 0.4572 - Val Acc: 83.83%\n",
      "Epoch 19/20 - Val Loss: 0.4562 - Val Acc: 84.06%\n",
      "Epoch 20/20 - Val Loss: 0.4522 - Val Acc: 84.20%\n"
     ]
    }
   ],
   "source": [
    "# Train model: train(self, X_train, y_train, X_val=None, y_val=None, epochs=100, batch_size=64)\n",
    "mlp1.train(train_images, y_train_one_hot, val_images, y_val_one_hot, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 84.20%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "predictions = mlp1.predict(val_images)\n",
    "accuracy = np.mean(predictions == val_labels) * 100\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
